const e=JSON.parse('{"key":"v-5e7edfb7","path":"/news/news20130903.html","title":"杜克大学john来我实验室交流学习","lang":"zh-CN","frontmatter":{"date":"2013-09-03T00:00:00.000Z","category":["未分类"],"editLink":false,"cover":null,"description":"杜克大学john来我实验室交流学习 John Paisley received the B.S.E. (2004), M.S. (2007) and Ph.D. (2010) in Electrical &amp; Computer Engineering from Duke University, where his advisor was Lawrence Carin. He was a postdoctoral researcher with David Blei in the Computer Science Department at Princeton University, and currently with Michael Jordan in the Department of EECS at UC Berkeley. He works on developing Bayesian models for machine learning applications, particularly for dictionary learning and topic modeling. Bayesian nonparametrics is an area in machine learning in which models grow in size and complexity as data accrue. As such, they they are particularly relevant to the world of \\"Big Data\\", where it may be difficult or even counterproductive to fix the number of parameters a priori. A stumbling block for Bayesian nonparametrics has been that their algorithms for posterior inference generally show poor scalability. In this talk, we tackle this issue in the domain of large-scale text collections. Our model is a novel tree-structured model in which documents are represented by collections of paths in an infinite-dimensional tree. We develop a general and efficient variational inference strategy for learning such models based on stochastic optimization, and show that with this combination of modeling and inference approach, we are able to learn high-quality models using millions of documents.","head":[["meta",{"property":"og:url","content":"https://xmu-smartdsp.github.io/news/news20130903.html"}],["meta",{"property":"og:site_name","content":"厦门大学智能数据分析与处理实验室"}],["meta",{"property":"og:title","content":"杜克大学john来我实验室交流学习"}],["meta",{"property":"og:description","content":"杜克大学john来我实验室交流学习 John Paisley received the B.S.E. (2004), M.S. (2007) and Ph.D. (2010) in Electrical &amp; Computer Engineering from Duke University, where his advisor was Lawrence Carin. He was a postdoctoral researcher with David Blei in the Computer Science Department at Princeton University, and currently with Michael Jordan in the Department of EECS at UC Berkeley. He works on developing Bayesian models for machine learning applications, particularly for dictionary learning and topic modeling. Bayesian nonparametrics is an area in machine learning in which models grow in size and complexity as data accrue. As such, they they are particularly relevant to the world of \\"Big Data\\", where it may be difficult or even counterproductive to fix the number of parameters a priori. A stumbling block for Bayesian nonparametrics has been that their algorithms for posterior inference generally show poor scalability. In this talk, we tackle this issue in the domain of large-scale text collections. Our model is a novel tree-structured model in which documents are represented by collections of paths in an infinite-dimensional tree. We develop a general and efficient variational inference strategy for learning such models based on stochastic optimization, and show that with this combination of modeling and inference approach, we are able to learn high-quality models using millions of documents."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-11-04T10:05:49.000Z"}],["meta",{"property":"article:author","content":"SmartDSP"}],["meta",{"property":"article:published_time","content":"2013-09-03T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-11-04T10:05:49.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"杜克大学john来我实验室交流学习\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2013-09-03T00:00:00.000Z\\",\\"dateModified\\":\\"2023-11-04T10:05:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"SmartDSP\\",\\"url\\":\\"https://xmu-smartdsp.github.io/\\"}]}"]]},"headers":[],"git":{"createdTime":1699084779000,"updatedTime":1699092349000,"contributors":[{"name":"huiiz","email":"1390113493@qq.com","commits":2}]},"readingTime":{"minutes":0.8,"words":239},"filePathRelative":"news/news20130903.md","localizedDate":"2013年9月3日","excerpt":"<h1> 杜克大学john来我实验室交流学习</h1>\\n<p>John Paisley received the B.S.E. (2004), M.S. (2007) and Ph.D. (2010) in<br>\\nElectrical &amp; Computer Engineering from Duke University, where his advisor was<br>\\nLawrence Carin. He was a postdoctoral researcher with David Blei in the<br>\\nComputer Science Department at Princeton University, and currently with<br>\\nMichael Jordan in the Department of EECS at UC Berkeley. He works on<br>\\ndeveloping Bayesian models for machine learning applications, particularly for<br>\\ndictionary learning and topic modeling. Bayesian nonparametrics is an area in<br>\\nmachine learning in which models grow in size and complexity as data accrue.<br>\\nAs such, they they are particularly relevant to the world of \\"Big Data\\", where<br>\\nit may be difficult or even counterproductive to fix the number of parameters<br>\\na priori. A stumbling block for Bayesian nonparametrics has been that their<br>\\nalgorithms for posterior inference generally show poor scalability. In this<br>\\ntalk, we tackle this issue in the domain of large-scale text collections. Our<br>\\nmodel is a novel tree-structured model in which documents are represented by<br>\\ncollections of paths in an infinite-dimensional tree. We develop a general and<br>\\nefficient variational inference strategy for learning such models based on<br>\\nstochastic optimization, and show that with this combination of modeling and<br>\\ninference approach, we are able to learn high-quality models using millions of<br>\\ndocuments.</p>\\n","autoDesc":true}');export{e as data};
