import{_ as e}from"./plugin-vue_export-helper-c27b6911.js";import{o as r,c as a,g as i,e as n}from"./app-87ca22f4.js";const t={},o=n('<h1 id="杜克大学john来我实验室交流学习" tabindex="-1"><a class="header-anchor" href="#杜克大学john来我实验室交流学习" aria-hidden="true">#</a> 杜克大学john来我实验室交流学习</h1><p>John Paisley received the B.S.E. (2004), M.S. (2007) and Ph.D. (2010) in<br> Electrical &amp; Computer Engineering from Duke University, where his advisor was<br> Lawrence Carin. He was a postdoctoral researcher with David Blei in the<br> Computer Science Department at Princeton University, and currently with<br> Michael Jordan in the Department of EECS at UC Berkeley. He works on<br> developing Bayesian models for machine learning applications, particularly for<br> dictionary learning and topic modeling. Bayesian nonparametrics is an area in<br> machine learning in which models grow in size and complexity as data accrue.<br> As such, they they are particularly relevant to the world of &quot;Big Data&quot;, where<br> it may be difficult or even counterproductive to fix the number of parameters<br> a priori. A stumbling block for Bayesian nonparametrics has been that their<br> algorithms for posterior inference generally show poor scalability. In this<br> talk, we tackle this issue in the domain of large-scale text collections. Our<br> model is a novel tree-structured model in which documents are represented by<br> collections of paths in an infinite-dimensional tree. We develop a general and<br> efficient variational inference strategy for learning such models based on<br> stochastic optimization, and show that with this combination of modeling and<br> inference approach, we are able to learn high-quality models using millions of<br> documents.</p>',2);function s(c,l){return r(),a("div",null,[o,i(" more ")])}const m=e(t,[["render",s],["__file","news20130903.html.vue"]]);export{m as default};
